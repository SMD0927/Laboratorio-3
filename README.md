# Problema del coctel
 LABORATORIO - 3 PROCESAMIENTO DIGITAL DE SEALES


## Requisitos
- Python 3.9
- Bibliotecas necesarias:
  - numpy
  - matplotlib
  - scipy.io
  - sounddevice
  - FastICA
 
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile
import sounddevice as sd
from sklearn.decomposition import FastICA
```

## Introducci贸n
El problema de la "fiesta de c贸ctel" es un fen贸meno estudiado en procesamiento de se帽ales que hace referencia a la capacidad de un sistema para concentrarse en una sola fuente sonora mientras filtra las dem谩s en un entorno con m煤ltiples emisores de sonido. Este problema es de gran relevancia en aplicaciones de mejora de la inteligibilidad del habla, reconocimiento autom谩tico de voz y cancelaci贸n de interferencias ac煤sticas en entornos ruidosos.

En este laboratorio, mediante un arreglo de  tres micr贸fonos estrategicamente distribuidos en el espacio se capturar谩n tres se帽ales de voz. Estas se帽ales adquiridas se analizar谩n de manera espectral utilizando transformada de Fourier discreta (DFT) o la transformada r谩pida de Fourier (FFT), describiendo la informaci贸n que se puede obtener con cada una de ellas. Adem谩s, se implementar谩n t茅cnicas previamente investigadas de separaci贸n de se帽ales como el An谩lisis de Componentes Independientes (ICA) o el Beamforming con el objetivo de aislar la se帽al de inter茅s a partir de las se帽ales capturadas por los micr贸fonos. 

A lo largo de este experimento,  se evaluar谩 el impacto de la disposici贸n espacial de los micr贸fonos en la efectividad de la separaci贸n de se帽ales, as铆 como analizar m茅tricas cuantitativas, como la relaci贸n se帽al-ruido (SNR), para medir el desempe帽o de los m茅todos aplicados.

---

## Importaci贸n de audios y ruidos 
```python
fs1, micro1 = wavfile.read("audioana.wav")
fsr1, ruido1 = wavfile.read("ruidoana.wav")

fs2, micro2 = wavfile.read("Audiosantiago.wav")
fsr2, ruido2 = wavfile.read("ruidosantiago.wav")

fs3, micro3 = wavfile.read("audiosamuel.wav")
fsr3, ruido3 = wavfile.read("ruidosamuel.wav")
```
Este c贸digo lee archivos de audio en formato WAV utilizando wavfile.read de scipy.io.wavfile. Y se cargan dos archivos: uno con la se帽al de las voces (micro) y otro con el ruido ambiental (ruido), junto con sus respectivas frecuencias de muestreo (fs y fsr).

### 1. Calculo Del SNR
```python
def SNR(m,r):
  potencia_se帽al = np.mean(m**2)
  potencia_ruido = np.mean(r**2)
  
  snr = 10 * np.log10(potencia_se帽al/potencia_ruido)
  return snr

print('SNR micr贸fono 1:',round(SNR(micro1,ruido1),3),'dB')
print('SNR micr贸fono 2:', round(SNR(micro2,ruido2),3),'dB')
print('SNR micr贸fono 2:', round(SNR(micro3,ruido3),3),'dB')
```
Este c贸digo calcula la relaci贸n se帽al-ruido (SNR) en decibeles (dB) para tres micr贸fonos. La funci贸n SNR(m, r) recibe dos arreglos NumPy: m, que representa la se帽al del micr贸fono, y r, que representa el ruido. Calcula la potencia de la se帽al y el ruido como la media de sus cuadrados y luego obtiene el SNR usando la f贸rmula.

$$
SNR = 10 \cdot \log_{10} \left( \frac{P_{\text{se帽al}}}{P_{\text{ruido}}} \right)
$$

Finalmente, se imprimen los valores de SNR redondeados a tres decimales para cada micr贸fono.

$$
SNR_{mic1} = 8.148 \text{ dB}
$$

$$
SNR_{mic2} = 14.267 \text{ dB}
$$

$$
SNR_{mic3} = 8.39 \text{ dB}
$$


### 2. Reproducci贸n de los audios
```python
def reproducir_audio(audio, fs):
    sd.play(audio, fs)
    sd.wait()

reproducir_audio(micro1, fs1)
reproducir_audio(micro2, fs2)
reproducir_audio(micro3, fs3)
```

El c贸digo define una funci贸n llamada reproducir_audio que toma un archivo de audio y su frecuencia de muestreo (fs) como par谩metros, reproduci茅ndolo con la biblioteca sounddevice. Luego, se llama a esta funci贸n tres veces para reproducir los audios micro1, micro2 y micro3 con sus respectivas frecuencias de muestreo (fs1, fs2, fs3). La funci贸n sd.wait() asegura que cada audio termine de reproducirse antes de iniciar el siguiente.

---
## An谩lisis Temporal 

### 1. Calculo de varianza y media
```python
    print('Media micr贸fono 1:', round(np.mean(micro1),3))
    print('Varianza micr贸fono 1:', round(np.var(micro1,ddof=1),3))
   
    print('Media micr贸fono 2:', round(np.mean(micro2),4))
    print('Varianza micr贸fono 2:', round(np.var(micro2,ddof=1),3))
   
    print('Media micr贸fono 3:', round(np.mean(micro3),3))
    print('Varianza micr贸fono 3:', round(np.var(micro3,ddof=1),3))
```
El c贸digo calcula y muestra la media y la varianza de los datos de los tres micr贸fonos (micro1, micro2 y micro3). Usa np.mean() para obtener la media y np.var(..., ddof=1) para la varianza muestral. Los resultados se redondean a tres o cuatro decimales antes de imprimirse.

$$
\mu_{mic1} = -0.025
$$

$$
\sigma^2_{mic1} = 16,529,446
$$

$$
\mu_{mic2} = -0.001
$$

$$
\sigma^2_{mic2} = 426,058
$$

$$
\mu_{mic3} = -0.003
$$

$$
\sigma^2_{mic3} = 6,089,800
$$



### 2. Grafica de los audios

El c贸digo genera y muestra las gr谩ficas de las se帽ales de audio captadas por los tres micr贸fonos. Para cada micr贸fono, se crea un vector de tiempo (t1, t2, t3) usando np.linspace(0, duraci贸n, n煤mero de muestras), donde la duraci贸n se obtiene dividiendo la cantidad de muestras (len(microX)) por la frecuencia de muestreo (fsX). Esto permite representar la se帽al en el dominio del tiempo. Luego, plt.plot() grafica cada se帽al.

```python
    t1 = np.linspace(0, len(micro1) / fs1, len(micro1))
    fig = plt.figure(figsize=(10, 5)) 
    plt.plot(t1,micro1,color='y')
    plt.title("Se帽al microfono 1")  
    plt.xlabel("tiempo [s]") 
    plt.ylabel("Amplitud digital (int16)") 
    plt.grid()
    
    t2 = np.linspace(0, len(micro2) / fs2, len(micro2))
    fig = plt.figure(figsize=(10, 5)) 
    plt.plot(t2,micro2,color='b')
    plt.title("Se帽al microfono 2")  
    plt.xlabel("tiempo [s]") 
    plt.ylabel("Amplitud digital (int16)") 
    plt.grid()
    
    t3 = np.linspace(0, len(micro3) / fs2, len(micro3))
    fig = plt.figure(figsize=(10, 5)) 
    plt.plot(t3,micro3,color='r')
    plt.title("Se帽al microfono 3")  
    plt.xlabel("tiempo [s]") 
    plt.ylabel("Amplitud digital (int16)") 
    plt.grid()
```


<p align="center">
    <img src="https://github.com/user-attachments/assets/e01e61e9-b2b7-4b56-895d-7daac9d08063" alt="image" width="450">
</p>

<p align="center">
    <img src="https://github.com/user-attachments/assets/52bfce35-f418-456f-b8c7-991cb7bd6e46" alt="image" width="450">
</p>

<p align="center">
    <img src="https://github.com/user-attachments/assets/8bb660e4-c995-426c-b580-9b63b38cfe7a" alt="image" width="450">
</p>

---
## An谩lisis Espectral

### 1. Transformada rapida de fourier
```python
frecuencias = np.fft.fftfreq(len(micro1), 1/fs1)
spectro = np.abs(np.fft.fft(micro1)) / len(micro1)
```

Este c贸digo realiza un an谩lisis de frecuencia de una se帽al de audio utilizando la Transformada R谩pida de Fourier (FFT). Primero, con np.fft.fftfreq(len(micro1), 1/fs1), se generan las frecuencias correspondientes a cada componente de la FFT, donde len(micro1) es el n煤mero de muestras de la se帽al y 1/fs1 es el periodo de muestreo, determinado por la frecuencia de muestreo fs1. Luego, con np.fft.fft(micro1), se calcula la FFT de la se帽al micro1, transform谩ndola del dominio del tiempo al dominio de la frecuencia. Posteriormente, se obtiene la magnitud del espectro con np.abs(...), descartando la informaci贸n de fase, y finalmente, se normaliza dividiendo entre len(micro1). Como resultado, se obtiene el espectro de frecuencias de la se帽al, lo que permite analizar su contenido en t茅rminos de amplitud y frecuencia.

### 2. Graficas

Este c贸digo genera y muestra el espectro de frecuencias de la se帽al micro1. Se grafica la mitad del espectro positivo con plt.plot(...), usando la frecuencia en el eje X y la amplitud normalizada en el eje Y. Cabe mencionar que es solo de la transformada de un se帽al, pero en el codigo se calculan la transformada de cada una de las tres se帽ales grabadas.

```python
fig = plt.figure(figsize=(8, 4)) 
plt.plot(frecuencias[:len(micro1)//2], spectro[:len(micro1)//2],color='y')
plt.title("Transformada de fourier de la se帽al 1 (espectro)")  
plt.xlabel("Frecuencias [Hz]") 
plt.ylabel("Amplitud Normalizada")
plt.grid()
plt.show()
```

<p align="center">
    <img src="https://github.com/user-attachments/assets/2d6e6627-0a52-4e5a-80dc-5114d79baa31" alt="image" width="450">
</p>

<p align="center">
    <img src="https://github.com/user-attachments/assets/372acc0f-31f2-4673-8322-478f1fa41777" alt="image" width="450">
</p>

<p align="center">
    <img src="https://github.com/user-attachments/assets/50bf3a22-1849-4a08-8e24-7cd5f565cc73" alt="image" width="450">
</p>




---
## Separaci贸n de Voces
```python
    archivos_audio = ["AudioSantiago.wav", "AudioSamuel.wav", "AudioAna.wav"]
    nombres = ["Santiago", "Samuel", "Ana"]
    
    audios = []
    fs_list = []
    
    # Cargar audios y recortar a la misma longitud
    for archivo in archivos_audio:
        fs, audio = cargar_audio(archivo)
        audios.append(audio)
        fs_list.append(fs)
    fs = fs_list[0]
    min_len = min(len(a) for a in audios)
    audios = [a[:min_len] for a in audios]
    
    # Construir la matriz de mezcla y convertir a float64
    X = np.c_[audios[0], audios[1], audios[2]].astype(np.float64)
    
    # Aplicar FastICA para extraer las componentes independientes
    ica = FastICA(n_components=3, random_state=0)
    S = ica.fit_transform(X)  # S tiene forma (n_samples, 3)
    
    # Para cada componente, aplicar filtros y guardar el audio resultante
    for i in range(3):
        cutoff = 300  # Filtro m铆nimo de 300 Hz
        audio_filtrado = highpass_filter(S[:, i], fs, cutoff, order=6)
        audio_filtrado = bandpass_filter(audio_filtrado, fs, lowcut=cutoff, highcut=8000, order=6)
        guardar_audio(f"voz_{nombres[i]}.wav", audio_filtrado, fs)
```
 Se define una lista de archivos (archivos_audio) y se recogen en dos listas: una para los audios y otra para las frecuencias de muestreo (fs_list).
Se recorta cada audio a la misma longitud (la m铆nima entre ellos) para asegurar que la matriz de mezcla sea compatible.
Construcci贸n de la matriz de mezcla:
Se combinan los audios en una matriz  usando np.c_[ ], donde cada columna representa una grabaci贸n.
La matriz se convierte a tipo float64 para asegurar precisi贸n en los c谩lculos.
Aplicaci贸n de FastICA:

 Se instancia FastICA con 3 componentes y un random_state fijo para obtener resultados reproducibles.
Se usa fit_transform sobre  para extraer las fuentes independientes. El resultado  es una matriz donde cada columna es una componente independiente que, idealmente, corresponde a una voz.
Filtrado y guardado de componentes:

 Para cada componente extra铆da se aplica primero un filtro pasa altos (para eliminar frecuencias bajas) y luego un filtro pasa banda (para limitar el rango entre 300 Hz y 8000 Hz).
Finalmente, se guarda cada componente filtrada en un archivo WAV nombrado seg煤n la voz (por ejemplo, voz_Santiago.wav).
Explicaci贸n de por qu茅 la separaci贸n podr铆a no funcionar como se esperaba:

- Cuando los micr贸fonos capturan mezclas muy parecidas (debido a la posici贸n de las fuentes o caracter铆sticas similares en la grabaci贸n), la matriz de mezcla resultante presenta poca diversidad. Esto dificulta que FastICA, que asume independencia y no-gaussianidad de las fuentes, pueda distinguirlas con precisi贸n.
- Una relaci贸n se帽al-ruido baja o un preprocesamiento deficiente afectan las estad铆sticas de las se帽ales. Dado que FastICA optimiza funciones de contraste (como la funci贸n 'logcosh' por defecto) para maximizar la independencia, cualquier contaminaci贸n o baja calidad de la se帽al puede degradar la separaci贸n.
- La calidad de la separaci贸n mejora cuando las mezclas tienen perfiles espaciales o espectrales diferenciados. Si estos perfiles son muy similares, incluso ajustando par谩metros como n_components, whiten (por defecto 'unit-variance') y fun (por ejemplo, 'logcosh'), la asignaci贸n de componentes a cada voz puede volverse inexacta, lo que resulta en una separaci贸n deficiente de las fuentes.
----
## Conclusi贸n



----
## Bibliografias

----
## Autores 
- Samuel Pe帽a
- Ana Abril
- Santiago Mora
